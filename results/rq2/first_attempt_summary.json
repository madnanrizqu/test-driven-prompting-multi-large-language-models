{
  "total_comparisons": 3,
  "td_better": 3,
  "td_same": 0,
  "td_worse": 0,
  "using_remediation": false,
  "details": {
    "code_contests_chatgpt4o_combined": {
      "status": "better",
      "base_accuracy": 3.71,
      "td_accuracy": 26.24,
      "using_remediation": false,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined_td"
    },
    "code_contests_claude35sonnet_combined": {
      "status": "better",
      "base_accuracy": 4.46,
      "td_accuracy": 39.85,
      "using_remediation": false,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined_td"
    },
    "code_contests_qwen25coder32b_combined": {
      "status": "better",
      "base_accuracy": 3.47,
      "td_accuracy": 32.67,
      "using_remediation": false,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined_td"
    }
  },
  "rq_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties",
  "results_folder": "dynamic",
  "timestamp": "2025-09-10 17:20:07",
  "incomplete_directories": {
    "missing_td_dirs": [],
    "missing_summary_files": [],
    "missing_accuracy_data": [],
    "partial_completion_dirs": [],
    "successful_comparisons": [
      "code_contests_chatgpt4o_combined",
      "code_contests_claude35sonnet_combined",
      "code_contests_qwen25coder32b_combined"
    ],
    "config_mismatch_dirs": [],
    "total_directories": 3
  },
  "accuracy_statistics": {
    "increases": [
      22.529999999999998,
      35.39,
      29.200000000000003
    ],
    "total_increase": 87.12,
    "avg_increase": 29.04,
    "median_increase": 29.200000000000003,
    "std_dev": 6.431492828263126,
    "min_increase": 22.529999999999998,
    "max_increase": 35.39,
    "percentile_25": 25.865000000000002,
    "percentile_75": 32.295,
    "interquartile_range": 6.43,
    "improved_count": 3,
    "worsened_count": 0,
    "same_count": 0,
    "avg_improvement_pct": 747.4246483192295,
    "avg_regression_pct": 0,
    "confidence_interval": [
      13.063286122657768,
      45.01671387734223
    ],
    "sorted_increases_desc": [
      {
        "benchmark": "code_contests_claude35sonnet_combined",
        "base_accuracy": 4.46,
        "td_accuracy": 39.85,
        "increase": 35.39,
        "pct_change": 793.4977578475336
      },
      {
        "benchmark": "code_contests_qwen25coder32b_combined",
        "base_accuracy": 3.47,
        "td_accuracy": 32.67,
        "increase": 29.200000000000003,
        "pct_change": 841.4985590778098
      },
      {
        "benchmark": "code_contests_chatgpt4o_combined",
        "base_accuracy": 3.71,
        "td_accuracy": 26.24,
        "increase": 22.529999999999998,
        "pct_change": 607.277628032345
      }
    ],
    "sorted_regressions_asc": [
      {
        "benchmark": "code_contests_chatgpt4o_combined",
        "base_accuracy": 3.71,
        "td_accuracy": 26.24,
        "increase": 22.529999999999998,
        "pct_change": 607.277628032345
      },
      {
        "benchmark": "code_contests_qwen25coder32b_combined",
        "base_accuracy": 3.47,
        "td_accuracy": 32.67,
        "increase": 29.200000000000003,
        "pct_change": 841.4985590778098
      },
      {
        "benchmark": "code_contests_claude35sonnet_combined",
        "base_accuracy": 4.46,
        "td_accuracy": 39.85,
        "increase": 35.39,
        "pct_change": 793.4977578475336
      }
    ],
    "normality_test_stat": 0.9995358293786154,
    "normality_p_value": 0.9588496241850586,
    "is_normal": true,
    "significance_test_type": "paired_t_test",
    "significance_test_stat": 7.820696810974097,
    "significance_p_value": 0.015959328816599574,
    "cohens_d": 6.0147561224588975,
    "effect_size_interpretation": "large"
  }
}