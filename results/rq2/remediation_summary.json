{
  "total_comparisons": 3,
  "td_better": 3,
  "td_same": 0,
  "td_worse": 0,
  "using_remediation": true,
  "details": {
    "code_contests_chatgpt4o_combined": {
      "status": "better",
      "base_accuracy": 24.26,
      "td_accuracy": 31.44,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_chatgpt4o_combined_td"
    },
    "code_contests_claude35sonnet_combined": {
      "status": "better",
      "base_accuracy": 51.24,
      "td_accuracy": 54.46,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_claude35sonnet_combined_td"
    },
    "code_contests_qwen25coder32b_combined": {
      "status": "better",
      "base_accuracy": 5.2,
      "td_accuracy": 36.14,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties/code_contests_qwen25coder32b_combined_td"
    }
  },
  "rq_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq2_difficulties",
  "results_folder": "dynamic",
  "timestamp": "2025-09-10 17:20:07",
  "incomplete_directories": {
    "missing_td_dirs": [],
    "missing_summary_files": [],
    "missing_accuracy_data": [],
    "partial_completion_dirs": [],
    "successful_comparisons": [
      "code_contests_chatgpt4o_combined",
      "code_contests_claude35sonnet_combined",
      "code_contests_qwen25coder32b_combined"
    ],
    "config_mismatch_dirs": [],
    "total_directories": 3
  },
  "accuracy_statistics": {
    "increases": [
      7.18,
      3.219999999999999,
      30.94
    ],
    "total_increase": 41.34,
    "avg_increase": 13.78,
    "median_increase": 7.18,
    "std_dev": 14.992318032912724,
    "min_increase": 3.219999999999999,
    "max_increase": 30.94,
    "percentile_25": 5.199999999999999,
    "percentile_75": 19.060000000000002,
    "interquartile_range": 13.860000000000003,
    "improved_count": 3,
    "worsened_count": 0,
    "same_count": 0,
    "avg_improvement_pct": 210.29339862479483,
    "avg_regression_pct": 0,
    "confidence_interval": [
      -23.462982611651483,
      51.022982611651486
    ],
    "sorted_increases_desc": [
      {
        "benchmark": "code_contests_qwen25coder32b_combined",
        "base_accuracy": 5.2,
        "td_accuracy": 36.14,
        "increase": 30.94,
        "pct_change": 595.0
      },
      {
        "benchmark": "code_contests_chatgpt4o_combined",
        "base_accuracy": 24.26,
        "td_accuracy": 31.44,
        "increase": 7.18,
        "pct_change": 29.59604286892003
      },
      {
        "benchmark": "code_contests_claude35sonnet_combined",
        "base_accuracy": 51.24,
        "td_accuracy": 54.46,
        "increase": 3.219999999999999,
        "pct_change": 6.2841530054644785
      }
    ],
    "sorted_regressions_asc": [
      {
        "benchmark": "code_contests_claude35sonnet_combined",
        "base_accuracy": 51.24,
        "td_accuracy": 54.46,
        "increase": 3.219999999999999,
        "pct_change": 6.2841530054644785
      },
      {
        "benchmark": "code_contests_chatgpt4o_combined",
        "base_accuracy": 24.26,
        "td_accuracy": 31.44,
        "increase": 7.18,
        "pct_change": 29.59604286892003
      },
      {
        "benchmark": "code_contests_qwen25coder32b_combined",
        "base_accuracy": 5.2,
        "td_accuracy": 36.14,
        "increase": 30.94,
        "pct_change": 595.0
      }
    ],
    "normality_test_stat": 0.8546511627906977,
    "normality_p_value": 0.2529696489658319,
    "is_normal": true,
    "significance_test_type": "paired_t_test",
    "significance_test_stat": 1.5919926508964337,
    "significance_p_value": 0.2523829452849286,
    "cohens_d": 0.7456351896647706,
    "effect_size_interpretation": "medium"
  }
}